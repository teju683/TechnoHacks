# -*- coding: utf-8 -*-
"""Heart Disease Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E7Moq8f6Sj8NYG5ARiW8AkjNag6QUAvI
"""

!pip install ucimlrepo

# Commented out IPython magic to ensure Python compatibility.
#import all essential libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

import warnings
warnings.filterwarnings('ignore')

from ucimlrepo import fetch_ucirepo

# fetch dataset
heart_disease = fetch_ucirepo(id=45)

# data (as pandas dataframes)
X = heart_disease.data.features
y = heart_disease.data.targets

# metadata
print(heart_disease.metadata)

# variable information
print(heart_disease.variables)

dataset = pd.read_csv('https://archive.ics.uci.edu/static/public/45/data.csv')
dataset.head()

"""Exploratory Data Analysis"""

y = dataset["num"]

sns.countplot(y)


target_temp = dataset.num.value_counts()

print(target_temp)

print("Percentage of patience without heart problems: "+str(round(target_temp[0]*100/303,2)))
print("Percentage of patience with heart problems: "+str(round(target_temp[1]*100/303,2)))

"""Analysing the 'Chest Pain Type' feature"""

dataset["cp"].unique()

sns.barplot(x=dataset["cp"],y=y)

"""
Analysing the FBS feature"""

dataset["fbs"].describe()

dataset["fbs"].unique()

sns.barplot(x=dataset["fbs"],y=y)

"""Analysing the restecg feature"""

dataset["restecg"].unique()

sns.barplot(x=dataset["restecg"],y=y)

"""Analysing the 'exang' feature"""

dataset["exang"].unique()

sns.barplot(x=dataset["exang"],y=y)

"""Analysing the Slope feature"""

dataset["slope"].unique()

sns.barplot(x=dataset["slope"],y=y)

"""Analysing the 'ca' feature"""

dataset["ca"].unique()

sns.countplot(dataset["ca"])

sns.barplot(x=dataset["ca"],y=y)

"""Analysing the 'thal' feature"""

dataset["thal"].unique()

sns.barplot(x=dataset["thal"],y=y)

"""Train Test split"""

from sklearn.model_selection import train_test_split

predictors = dataset.drop("num",axis=1)
target = dataset["num"]

X_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.20,random_state=0)

X_train.shape

X_test.shape

Y_train.shape

Y_test.shape

"""Model Fitting"""

from sklearn.metrics import accuracy_score

"""Logistic Regression"""

import pandas as pd
from sklearn.linear_model import LogisticRegression

missing_values = X_train.isnull().sum()
print(missing_values)

X_train = X_train.dropna()
Y_train = Y_train[X_train.index]

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")
X_train = imputer.fit_transform(X_train)

lr = LogisticRegression()
lr.fit(X_train, Y_train)

X_test.isnull().sum().sum()

X_test = X_test.dropna()

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="mean")
X_test = imputer.fit_transform(X_test)

Y_pred_lr = lr.predict(X_test)

Y_pred_lr.shape

!pip install scikit-learn

from sklearn.metrics import accuracy_score

print(Y_pred_lr.shape)
print(Y_test.shape)

if Y_pred_lr.shape[0] > Y_test.shape[0]:
    Y_pred_lr = Y_pred_lr[:Y_test.shape[0]]
else:
    Y_test = Y_test[:Y_pred_lr.shape[0]]

score_lr = round(accuracy_score(Y_pred_lr,Y_test)*100,2)

print("The accuracy score achieved using Logistic Regression is: "+str(score_lr)+" %")

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()

nb.fit(X_train,Y_train)

Y_pred_nb = nb.predict(X_test)

Y_pred_nb.shape

print(len(Y_pred_nb))
print(len(Y_test))

# Truncate the longer array
if len(Y_pred_nb) > len(Y_test):
    Y_pred_nb = Y_pred_nb[:len(Y_test)]
elif len(Y_test) > len(Y_pred_nb):
    Y_test = Y_test[:len(Y_pred_nb)]

# Calculate the accuracy score
score_nb = round(accuracy_score(Y_pred_nb, Y_test) * 100, 2)

# Print the accuracy score
print("The accuracy score achieved using Naive Bayes is: " + str(score_nb) + " %")

"""
SVM"""

from sklearn import svm

sv = svm.SVC(kernel='linear')

sv.fit(X_train, Y_train)

Y_pred_svm = sv.predict(X_test)

Y_pred_svm.shape

from sklearn.metrics import accuracy_score

print(Y_pred_svm.shape)
print(Y_test.shape)

if len(Y_pred_svm) > len(Y_test):
    Y_pred_svm = Y_pred_svm[:len(Y_test)]
elif len(Y_test) > len(Y_pred_svm):
    Y_test = Y_test[:len(Y_pred_nb)]

# Calculate the accuracy score
score_svm = round(accuracy_score(Y_pred_svm, Y_test) * 100, 2)

# Print the accuracy score
print("The accuracy score achieved using SVM is: " + str(score_svm) + " %")

"""K Nearest Neighbors"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train,Y_train)
Y_pred_knn=knn.predict(X_test)

Y_pred_knn.shape

if len(Y_pred_knn) > len(Y_test):
    Y_pred_knn = Y_pred_knn[:len(Y_test)]
elif len(Y_test) > len(Y_pred_knn):
    Y_test = Y_test[:len(Y_pred_knn)]

# Calculate the accuracy score
score_knn = round(accuracy_score(Y_pred_knn, Y_test) * 100, 2)

# Print the accuracy score
print("The accuracy score achieved using KNN is: " + str(score_knn) + " %")

"""Decision Tree"""

print(Y_pred_dt.shape)
print(Y_test.shape)

difference = Y_pred_dt.shape[0] - Y_test.shape[0]
print(difference)

Y_pred_dt = Y_pred_dt[:Y_test.shape[0]]

current_accuracy = round(accuracy_score(Y_pred_dt,Y_test)*100,2)

print(Y_pred_dt.shape)

score_dt = round(accuracy_score(Y_pred_dt,Y_test)*100,2)

print("The accuracy score achieved using Decision Tree is: "+str(score_dt)+" %")

"""XGBoost

"""

import xgboost as xgb
from sklearn.metrics import accuracy_score

xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
xgb_model.fit(X_train, Y_train)

Y_pred_xgb = xgb_model.predict(X_test)

print(Y_pred_xgb.shape)
print(Y_test.shape)

Y_pred_xgb.shape

print(f"Length of Y_pred_xgb: {len(Y_pred_xgb)}")
print(f"Length of Y_test: {len(Y_test)}")

Y_pred_xgb = Y_pred_xgb[:-1]

import numpy as np

# Convert the lists to numpy arrays
Y_pred_xgb = np.array(Y_pred_xgb)
Y_test = np.array(Y_test)

# Check the shape of the arrays
print(Y_pred_xgb.shape)
print(Y_test.shape)

# Now the code will execute without errors
if Y_pred_xgb.shape[0] > Y_test.shape[0]:
    Y_pred_xgb = Y_pred_xgb[:Y_test.shape[0]]
elif Y_test.shape[0] > Y_pred_xgb.shape[0]:
    Y_test = Y_test[:Y_pred_xgb.shape[0]]

if Y_pred_xgb.shape[0] > Y_test.shape[0]:
    Y_pred_xgb = Y_pred_xgb[:Y_test.shape[0]]
elif Y_test.shape[0] > Y_pred_xgb.shape[0]:
    Y_test = Y_test[:Y_pred_xgb.shape[0]]

score_xgb = round(accuracy_score(Y_pred_xgb, Y_test) * 100, 2)

print("The accuracy score achieved using XGBoost is: " + str(score_xgb) + " %")